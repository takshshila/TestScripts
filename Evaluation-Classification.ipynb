{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "428046b7-a032-4a2d-91c7-f6f6c4aa4a7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import requests\n",
    "import json\n",
    "import os\n",
    "import paho.mqtt.client as client\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "import traceback\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "99b3f4b6-2730-4261-8bc8-df5500df9e6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def process_bin_clas(df_train, df_test):\n",
    "#     df_total = pd.concat([df_train, df_test], axis=0)\n",
    "#     tar = df_total.iloc[:, -1]\n",
    "#     df_total = df_total.iloc[:, :-1]\n",
    "#     cat = df_total.select_dtypes(include=['object']).columns.to_list()\n",
    "#     df_total = pd.get_dummies(df_total, cat)\n",
    "#     df_total = pd.concat([df_total, tar], axis=1)\n",
    "#     df_train = df_total.iloc[:df_train.shape[0],:]\n",
    "#     df_test = df_total.iloc[df_train.shape[0]:,:]\n",
    "    \n",
    "#     return df_train, df_test\n",
    "\n",
    "# def get_missing():\n",
    "#     missingValueColumns = datafrm.columns[datafrm.isnull().any()].tolist()\n",
    "#     percent_missing = datafrm[missingValueColumns].isnull().sum()\n",
    "#     print(\"Missing value count columnwise:\")\n",
    "#     print(percent_missing)\n",
    "\n",
    "\n",
    "# data_dir =\\\n",
    "#     r\"C:\\Users\\arpit\\workspace\\setup-stuff\\gateway_and_dataset\\Classification\\DATASETS_CLASSIFICATION\\BINARY_PROBLEMS\"\n",
    "# datasets = [\"Adult\", \"Breast Cancer\", \"Credit Screening\", \"Ionosphere\", \"Liver Disorder\", \"Pima Indian\", \"Sonar\"]\n",
    "# shorts = [\"AD\", \"BC\", \"CR\", \"IO\", \"LD\", \"PI\", \"SN\"]\n",
    "# seps = [\",\", \"\\t\", \"\\t\", \"\\t\", \"\\t\", \"\\t\", \",\"]\n",
    "# header = [False, False, False, False, False, False, False]\n",
    "\n",
    "# seeds = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
    "# for i in range(len(datasets)):\n",
    "#     for seed in tqdm(seeds):\n",
    "#         dataset = datasets[i]\n",
    "#         short = shorts[i]\n",
    "        \n",
    "#         csv_path_train = os.path.join(data_dir, dataset, short + 'Train' + str(seed) + 'N.txt')\n",
    "#         csv_path_test = os.path.join(data_dir, dataset, short + 'Test' + str(seed) + 'N.txt')\n",
    "#         if header[i]:\n",
    "#             df_train = pd.read_csv(csv_path_train, sep = seps[i])\n",
    "#             df_test = pd.read_csv(csv_path_test, sep = seps[i])\n",
    "#         else:\n",
    "#             df_train = pd.read_csv(csv_path_train, sep = seps[i], header=None)\n",
    "#             df_test = pd.read_csv(csv_path_test, sep = seps[i], header=None)\n",
    "\n",
    "#         df_train, df_test = process_bin_clas(df_train, df_test)\n",
    "#         x, y = df_train.iloc[: , :-1], df_train.iloc[: , -1]\n",
    "#         x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.10, random_state=4, stratify=y)\n",
    "#         train_part_k = pd.concat([x_train, y_train], axis=1)\n",
    "#         train_part_rest = pd.concat([x_test, y_test], axis=1)\n",
    "        \n",
    "#         train_part_k.to_csv(os.path.join(data_dir, dataset, short + 'Train' + str(seed) + 'N_mod.txt'), index=False)\n",
    "#         train_part_rest.to_csv(os.path.join(data_dir, dataset, short + 'Val' + str(seed) + 'N_mod.txt'), index=False)\n",
    "#         df_test.to_csv(os.path.join(data_dir, dataset, short + 'Test' + str(seed) + 'N_mod.txt'), index=False)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ade4740b-1273-4179-a4ab-39f92dc92ebd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_prereq_results(target):\n",
    "    body = {\"algorithm\": \"classification\", \"target\": target}\n",
    "    url = 'http://localhost:5000/pre_req'\n",
    "    r = requests.post(url, json=body)\n",
    "    prereq_res = r.json()\n",
    "#     print(prereq_res.keys())\n",
    "    return prereq_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2c021075-18a3-487e-9c07-b66b4d8adaeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_phase0_results():\n",
    "    body = {\"algorithm\": \"classification\",\"chunk_count\": 30}\n",
    "    url = 'http://localhost:5000/phase0'\n",
    "    r = requests.post(url, json=body)\n",
    "    phase0_res = r.json()\n",
    "#     print(phase0_res.keys())\n",
    "    return phase0_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1988a1ec-c587-4705-b8de-8e6207a041ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_phase1_results(n_c=2, training_csv=None):\n",
    "    body = {\n",
    "        \"algorithm\": \"classification\",\n",
    "        \"Num_Classes\": n_c,\n",
    "        \"NUM_ITERATIONS\": 4,\n",
    "        \"NET_LATTICE_PHASE_1\": [3, 4, 5, 6, 7, 8],\n",
    "        \"no_data_Passes_kernel_Phase_1\": 10,\n",
    "        \"SIZE_OF_PARTITIONS\": 100,\n",
    "        \"NUM_DATASET_PASSES\": 50,\n",
    "        \"LEARNING_RATES_PHASE_1\": [0.05, 0.01, 0.005, 0.1],\n",
    "        \"CHUNK_SIZE\": 50,\n",
    "        \"training_csv\": training_csv\n",
    "    }\n",
    "    url = 'http://localhost:5000/phase1'\n",
    "    r = requests.post(url, json=body)\n",
    "    phase1_res = r.json()\n",
    "#     print(phase1_res)\n",
    "    return phase1_res\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "61b81706-be0d-40e0-a747-a0a7a1c3a32e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_phase2_results(frbc,\n",
    "                       validation_csv=None,\n",
    "                       training_csv=None,\n",
    "                       n_c=2,\n",
    "                       chunk_size=100\n",
    "                      ):\n",
    "    body = {\n",
    "        \"algorithm\": \"classification\",\n",
    "        \"Num_Classes\": n_c,\n",
    "        'FeatureRankingByClass': frbc,\n",
    "        \"CHUNK_SIZE\": chunk_size,\n",
    "        \"NoOfModels_Ensemble\": 3,\n",
    "        \"Max_FeatureSpaces_Size\": 50,\n",
    "        \"train_crossvalidation_percentage\": 1,\n",
    "        \"LEARNING_RATES_PHASE_2\": [0.05, 0.001, 0.08, 0.1],\n",
    "        \"NET_LATTICE_PHASE_2\": [3, 4, 5, 6, 7, 8],\n",
    "        \"no_data_Passes_kernel_Phase_2\": 10,\n",
    "        \"NUM_DATASET_PASSES\": 100,\n",
    "        \"validation_csv\": validation_csv,\n",
    "        \"training_csv\": training_csv\n",
    "    }\n",
    "    url = 'http://localhost:5000/phase2'\n",
    "    r = requests.post(url, json=body)\n",
    "    phase2_res = r.json()\n",
    "    return phase2_res\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9dceb34c-63c0-4a60-9307-01bbfc0c7ab0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_inference_results(testing_file):\n",
    "    body = {\n",
    "        \"algorithm\": \"classification\",\n",
    "        \"testing_file\": testing_file\n",
    "    }\n",
    "    url = 'http://localhost:5000/inference'\n",
    "    r = requests.post(url, json=body)\n",
    "    inference_res = r.json()\n",
    "    return inference_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "88228e38-09a0-422a-863d-9d88dd2301f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def process_results(data_directory, dataset, seed, short, sep=','):\n",
    "#     proc, gateway_proc, details = None, None, {}\n",
    "#     try:\n",
    "#         gateway_proc = subprocess.Popen(['python', os.path.join(data_directory, 'gateway_simulation.py')])\n",
    "#         backend_proc = subprocess.Popen(['python', r\"C:\\Users\\arpit\\workspace\\MainAppCuda\\main.py\"])\n",
    "#         proc = subprocess.Popen(['python', os.path.join(data_directory, 'sonar_stream_V2.py'), dataset, str(seed), short, sep])\n",
    "        \n",
    "#         data_dir = r\"C:\\Users\\arpit\\workspace\\setup-stuff\\gateway_and_dataset\\Classification\\DATASETS_CLASSIFICATION\\BINARY_PROBLEMS\"\n",
    "#         csv_path_train = os.path.join(data_dir, dataset, short + 'Train' + str(seed) + 'N_mod.txt')\n",
    "#         csv_path_test = os.path.join(data_dir, dataset, short + 'Test' + str(seed) + 'N_mod.txt')\n",
    "#         csv_path_val = os.path.join(data_dir, dataset, short + 'Val' + str(seed) + 'N_mod.txt')\n",
    "        \n",
    "#         train_df = pd.read_csv(csv_path_train, sep=sep)\n",
    "#         target = list(train_df.iloc[:, -1].unique())\n",
    "#         target = [str(tar) for tar in target]\n",
    "        \n",
    "#         prereq_res = get_prereq_results(target)\n",
    "#         phase0_res = get_phase0_results()\n",
    "#         phase1_res = get_phase1_results(len(target), csv_path_train)\n",
    "#         phase2_res = get_phase2_results(phase1_res[\"class\"], csv_path_val, csv_path_train)\n",
    "#         inference_res = get_inference_results(csv_path_test)\n",
    "        \n",
    "#         features = min(len(phase1_res[\"class\"][0][\"rankings\"]), 15)\n",
    "#         details = {'dataset': dataset, 'seed': seed, 'KernelTime': phase2_res['kernelTime'], 'hostTime': phase2_res['hostTime']}\n",
    "#         details['phase1_chunks_for_convergence'] = phase1_res['chunks_for_convergence']\n",
    "#         details['phase2_chunks_for_convergence'] = phase2_res['chunks_for_convergence_phase2']\n",
    "#         details['best_feature_list'] = phase2_res['minFeatureSpace']\n",
    "#         details['best_lr'] = phase2_res['learning_rate']\n",
    "#         details['best_fs'] = len(phase2_res['minFeatureSpace'])\n",
    "#         details['model_id'] = phase2_res['model_index']\n",
    "#         details['test_accuracy'] = inference_res['accuracyScore']\n",
    "#         details['test_precision'] = inference_res['precision']\n",
    "#         details['test_recall'] = inference_res['recall']\n",
    "#         details['test_fscore'] = inference_res['fscore']\n",
    "        \n",
    "#         details['train_accuracy'] = phase2_res['accuracyScore_t']\n",
    "#         details['train_precision'] = phase2_res['precision_t']\n",
    "#         details['train_recall'] = phase2_res['recall_t']\n",
    "#         details['train_fscore'] = phase2_res['fscore_t']\n",
    "#         details['val_accuracy'] = phase2_res['accuracyScore']\n",
    "#         details['val_precision'] = phase2_res['precision']\n",
    "#         details['val_recall'] = phase2_res['recall']\n",
    "#         details['val_fscore'] = phase2_res['fscore']\n",
    "        \n",
    "#         imp_features = []\n",
    "#         for k in range(2):\n",
    "#             for i in range(features):\n",
    "#                 imp_features.append(phase1_res[\"class\"][k][\"rankings\"][i]['Feature'])\n",
    "#             details['important_features_class_'+ str(k)] = imp_features\n",
    "#             imp_features = []\n",
    "#         backend_proc.kill()\n",
    "#         proc.kill()\n",
    "#         gateway_proc.kill()\n",
    "#     except:\n",
    "#         traceback.print_exc()\n",
    "#         if proc:\n",
    "#             proc.kill()\n",
    "#         if gateway_proc:\n",
    "#             gateway_proc.kill()\n",
    "#         if backend_proc:\n",
    "#             backend_proc.kill()\n",
    "#     return details\n",
    "\n",
    "\n",
    "# dataset_dir =\\\n",
    "#     r\"C:\\Users\\arpit\\workspace\\setup-stuff\\gateway_and_dataset\"\n",
    "# datasets = [\"Adult\", \"Breast Cancer\", \"Credit Screening\", \"Ionosphere\", \"Liver Disorder\", \"Pima Indian\", \"Sonar\"]\n",
    "# shorts = [\"AD\", \"BC\", \"CR\", \"IO\", \"LD\", \"PI\", \"SN\"] \n",
    "# header = [False, False, False, False, False, False, False]\n",
    "# datasets = [\"Ionosphere\"]\n",
    "# shorts = [\"IO\"]\n",
    "\n",
    "# seeds = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
    "# for i in range(len(datasets)):\n",
    "    \n",
    "#     df = pd.DataFrame()\n",
    "#     for seed in tqdm(seeds):\n",
    "#         dataset = datasets[i]\n",
    "#         short = shorts[i]\n",
    "#         details = process_results(dataset_dir, dataset, seed, short)\n",
    "#         df = df.append(details, ignore_index=True)\n",
    "#         filepath = Path(dataset + 'highConf_.csv')\n",
    "#         filepath.parent.mkdir(parents=True, exist_ok=True)\n",
    "#         df.to_csv(filepath, index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "896f20ea-5c20-4112-af20-58011100aef2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data11_cns_splits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 10/10 [00:15<00:00,  1.57s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data15_occupanydetection_splits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 10/10 [00:01<00:00,  7.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data19_musk_splits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 10/10 [00:03<00:00,  2.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data22_connect_splits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 10/10 [00:20<00:00,  2.09s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data26_sales_splits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 10/10 [00:03<00:00,  2.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data2_letter_splits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 10/10 [00:00<00:00, 11.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data6_shuttle_splits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 10/10 [00:01<00:00,  6.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data16_poker_splits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 10/10 [00:00<00:00, 10.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data1_credit_splits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 10/10 [00:02<00:00,  4.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data23_forestcover_splits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 10/10 [00:51<00:00,  5.11s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data27_yeast_splits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 10/10 [00:02<00:00,  3.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data7_penbased_splits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 10/10 [00:00<00:00, 14.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data13_lymphoma_splits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 10/10 [00:06<00:00,  1.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data17_skinseg_splits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 10/10 [00:03<00:00,  2.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data20_carvana_splits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 10/10 [04:11<00:00, 25.11s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data24_voice_splits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 10/10 [00:00<00:00, 10.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data28_theorem_splits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 10/10 [00:02<00:00,  4.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data4_activity_splits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 10/10 [00:11<00:00,  1.17s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data8_income_splits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 10/10 [00:06<00:00,  1.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data10_brain_splits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 10/10 [00:08<00:00,  1.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data14_prostrate_splits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 10/10 [00:13<00:00,  1.30s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data18_challenges_splits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 10/10 [00:23<00:00,  2.37s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data21_gimmecredit_splits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 10/10 [00:05<00:00,  1.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data29_dota_splits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 10/10 [00:17<00:00,  1.72s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data5_bank_splits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 10/10 [00:01<00:00,  5.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data9_amlall_splits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 10/10 [00:09<00:00,  1.03it/s]\n"
     ]
    }
   ],
   "source": [
    "def process_bin_clas(df_train, df_test):\n",
    "    df_total = pd.concat([df_train, df_test], axis=0)\n",
    "#     print(\"Merged shape:\", df_total.shape)\n",
    "    tar = df_total.iloc[:, -1]\n",
    "    df_total = df_total.iloc[:, :-1]\n",
    "    cat = df_total.select_dtypes(include=['object']).columns.to_list()\n",
    "    df_total = pd.get_dummies(df_total, cat)\n",
    "#     print(\"After dummies shape:\", df_total.shape)\n",
    "    df_total = pd.concat([df_total, tar], axis=1)\n",
    "#     print(\"After concat shape:\", df_total.shape)\n",
    "    df_train = df_total.iloc[:df_train.shape[0],:]\n",
    "    df_test = df_total.iloc[df_train.shape[0]:,:]\n",
    "    \n",
    "    return df_train, df_test\n",
    "\n",
    "def get_missing():\n",
    "    missingValueColumns = datafrm.columns[datafrm.isnull().any()].tolist()\n",
    "    percent_missing = datafrm[missingValueColumns].isnull().sum()\n",
    "    print(\"Missing value count columnwise:\")\n",
    "    print(percent_missing)\n",
    "\n",
    "\n",
    "data_dir =\\\n",
    "    r\"C:\\Users\\arpit\\workspace\\setup-stuff\\gateway_and_dataset\\Classification\\DATASETS_CLASSIFICATION\"\n",
    "datasets = ['data11_cns_splits','data15_occupanydetection_splits','data19_musk_splits','data22_connect_splits',\n",
    "            'data26_sales_splits','data2_letter_splits','data6_shuttle_splits', # 'data12_srbct_splits',\n",
    "            'data16_poker_splits','data1_credit_splits','data23_forestcover_splits','data27_yeast_splits',\n",
    "            'data7_penbased_splits','data13_lymphoma_splits','data17_skinseg_splits', 'data20_carvana_splits',\n",
    "            'data24_voice_splits','data28_theorem_splits','data4_activity_splits', 'data8_income_splits',\n",
    "            'data10_brain_splits','data14_prostrate_splits','data18_challenges_splits', 'data21_gimmecredit_splits',\n",
    "            'data29_dota_splits','data5_bank_splits','data9_amlall_splits'] # 'data25_arrythmia_splits',\n",
    "seeds = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
    "\n",
    "for i in range(len(datasets)):\n",
    "    print(datasets[i])\n",
    "    for seed in tqdm(seeds):\n",
    "        dataset = datasets[i]\n",
    "        remove_split = \"_\".join(dataset.split(\"_\")[:-1])\n",
    "        csv_path_train = os.path.join(data_dir, dataset, remove_split + '_train' + str(seed) + '.csv')\n",
    "        csv_path_test = os.path.join(data_dir, dataset, remove_split + '_test' + str(seed) + '.csv')\n",
    "        df_train = pd.read_csv(csv_path_train)\n",
    "        df_test = pd.read_csv(csv_path_test)\n",
    "#         print(\"Before\", \"Train\", df_train.shape, \"Test\", df_test.shape)\n",
    "        df_train, df_test = process_bin_clas(df_train, df_test)\n",
    "#         print(\"End\", \"Train\", df_train.shape, \"Test\", df_test.shape)\n",
    "        x, y = df_train.iloc[: , :-1], df_train.iloc[: , -1]\n",
    "        if dataset == \"data18_challenges_splits\":\n",
    "            x, y = df_train.iloc[: , 1:], df_train.iloc[: , 0]\n",
    "        x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.20, random_state=4, stratify=y)\n",
    "        train_part_k = pd.concat([x_train, y_train], axis=1)\n",
    "        train_part_rest = pd.concat([x_test, y_test], axis=1)\n",
    "        \n",
    "#         train_part_k.to_csv(os.path.join(data_dir, dataset, remove_split + '_train' + str(seed) + '_mod.csv'), index=False)\n",
    "        df_train.to_csv(os.path.join(data_dir, dataset, remove_split + '_train' + str(seed) + '_mod.csv'), index=False)\n",
    "        train_part_rest.to_csv(os.path.join(data_dir, dataset, remove_split + '_val' + str(seed) + '_mod.csv'), index=False)\n",
    "        df_test.to_csv(os.path.join(data_dir, dataset, remove_split + '_test' + str(seed) + '_mod.csv'), index=False)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4100b498",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████| 10/10 [12:57<00:00, 77.74s/it]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████| 10/10 [21:34<00:00, 129.50s/it]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████| 10/10 [2:57:40<00:00, 1066.08s/it]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████| 10/10 [2:46:12<00:00, 997.28s/it]\n",
      "  0%|                                                                                                        | 0/10 [00:00<?, ?it/s]Traceback (most recent call last):\n",
      "  File \"<ipython-input-9-dda364e21729>\", line 22, in process_results\n",
      "    inference_res = get_inference_results(csv_path_test)\n",
      "  File \"<ipython-input-7-488d7d0cc65c>\", line 8, in get_inference_results\n",
      "    inference_res = r.json()\n",
      "  File \"C:\\Users\\arpit\\anaconda3\\lib\\site-packages\\requests\\models.py\", line 900, in json\n",
      "    return complexjson.loads(self.text, **kwargs)\n",
      "  File \"C:\\Users\\arpit\\anaconda3\\lib\\json\\__init__.py\", line 357, in loads\n",
      "    return _default_decoder.decode(s)\n",
      "  File \"C:\\Users\\arpit\\anaconda3\\lib\\json\\decoder.py\", line 337, in decode\n",
      "    obj, end = self.raw_decode(s, idx=_w(s, 0).end())\n",
      "  File \"C:\\Users\\arpit\\anaconda3\\lib\\json\\decoder.py\", line 355, in raw_decode\n",
      "    raise JSONDecodeError(\"Expecting value\", s, err.value) from None\n",
      "json.decoder.JSONDecodeError: Expecting value: line 1 column 1 (char 0)\n",
      " 10%|█████████▎                                                                                   | 1/10 [08:12<1:13:50, 492.33s/it]Traceback (most recent call last):\n",
      "  File \"<ipython-input-9-dda364e21729>\", line 22, in process_results\n",
      "    inference_res = get_inference_results(csv_path_test)\n",
      "  File \"<ipython-input-7-488d7d0cc65c>\", line 8, in get_inference_results\n",
      "    inference_res = r.json()\n",
      "  File \"C:\\Users\\arpit\\anaconda3\\lib\\site-packages\\requests\\models.py\", line 900, in json\n",
      "    return complexjson.loads(self.text, **kwargs)\n",
      "  File \"C:\\Users\\arpit\\anaconda3\\lib\\json\\__init__.py\", line 357, in loads\n",
      "    return _default_decoder.decode(s)\n",
      "  File \"C:\\Users\\arpit\\anaconda3\\lib\\json\\decoder.py\", line 337, in decode\n",
      "    obj, end = self.raw_decode(s, idx=_w(s, 0).end())\n",
      "  File \"C:\\Users\\arpit\\anaconda3\\lib\\json\\decoder.py\", line 355, in raw_decode\n",
      "    raise JSONDecodeError(\"Expecting value\", s, err.value) from None\n",
      "json.decoder.JSONDecodeError: Expecting value: line 1 column 1 (char 0)\n",
      " 20%|██████████████████▌                                                                          | 2/10 [16:31<1:06:09, 496.23s/it]Traceback (most recent call last):\n",
      "  File \"<ipython-input-9-dda364e21729>\", line 22, in process_results\n",
      "    inference_res = get_inference_results(csv_path_test)\n",
      "  File \"<ipython-input-7-488d7d0cc65c>\", line 8, in get_inference_results\n",
      "    inference_res = r.json()\n",
      "  File \"C:\\Users\\arpit\\anaconda3\\lib\\site-packages\\requests\\models.py\", line 900, in json\n",
      "    return complexjson.loads(self.text, **kwargs)\n",
      "  File \"C:\\Users\\arpit\\anaconda3\\lib\\json\\__init__.py\", line 357, in loads\n",
      "    return _default_decoder.decode(s)\n",
      "  File \"C:\\Users\\arpit\\anaconda3\\lib\\json\\decoder.py\", line 337, in decode\n",
      "    obj, end = self.raw_decode(s, idx=_w(s, 0).end())\n",
      "  File \"C:\\Users\\arpit\\anaconda3\\lib\\json\\decoder.py\", line 355, in raw_decode\n",
      "    raise JSONDecodeError(\"Expecting value\", s, err.value) from None\n",
      "json.decoder.JSONDecodeError: Expecting value: line 1 column 1 (char 0)\n",
      " 30%|████████████████████████████▌                                                                  | 3/10 [24:32<57:05, 489.39s/it]Traceback (most recent call last):\n",
      "  File \"<ipython-input-9-dda364e21729>\", line 22, in process_results\n",
      "    inference_res = get_inference_results(csv_path_test)\n",
      "  File \"<ipython-input-7-488d7d0cc65c>\", line 8, in get_inference_results\n",
      "    inference_res = r.json()\n",
      "  File \"C:\\Users\\arpit\\anaconda3\\lib\\site-packages\\requests\\models.py\", line 900, in json\n",
      "    return complexjson.loads(self.text, **kwargs)\n",
      "  File \"C:\\Users\\arpit\\anaconda3\\lib\\json\\__init__.py\", line 357, in loads\n",
      "    return _default_decoder.decode(s)\n",
      "  File \"C:\\Users\\arpit\\anaconda3\\lib\\json\\decoder.py\", line 337, in decode\n",
      "    obj, end = self.raw_decode(s, idx=_w(s, 0).end())\n",
      "  File \"C:\\Users\\arpit\\anaconda3\\lib\\json\\decoder.py\", line 355, in raw_decode\n",
      "    raise JSONDecodeError(\"Expecting value\", s, err.value) from None\n",
      "json.decoder.JSONDecodeError: Expecting value: line 1 column 1 (char 0)\n",
      " 40%|██████████████████████████████████████                                                         | 4/10 [32:43<49:00, 490.06s/it]Traceback (most recent call last):\n",
      "  File \"<ipython-input-9-dda364e21729>\", line 22, in process_results\n",
      "    inference_res = get_inference_results(csv_path_test)\n",
      "  File \"<ipython-input-7-488d7d0cc65c>\", line 8, in get_inference_results\n",
      "    inference_res = r.json()\n",
      "  File \"C:\\Users\\arpit\\anaconda3\\lib\\site-packages\\requests\\models.py\", line 900, in json\n",
      "    return complexjson.loads(self.text, **kwargs)\n",
      "  File \"C:\\Users\\arpit\\anaconda3\\lib\\json\\__init__.py\", line 357, in loads\n",
      "    return _default_decoder.decode(s)\n",
      "  File \"C:\\Users\\arpit\\anaconda3\\lib\\json\\decoder.py\", line 337, in decode\n",
      "    obj, end = self.raw_decode(s, idx=_w(s, 0).end())\n",
      "  File \"C:\\Users\\arpit\\anaconda3\\lib\\json\\decoder.py\", line 355, in raw_decode\n",
      "    raise JSONDecodeError(\"Expecting value\", s, err.value) from None\n",
      "json.decoder.JSONDecodeError: Expecting value: line 1 column 1 (char 0)\n",
      " 50%|███████████████████████████████████████████████▌                                               | 5/10 [41:01<41:03, 492.74s/it]Traceback (most recent call last):\n",
      "  File \"<ipython-input-9-dda364e21729>\", line 22, in process_results\n",
      "    inference_res = get_inference_results(csv_path_test)\n",
      "  File \"<ipython-input-7-488d7d0cc65c>\", line 8, in get_inference_results\n",
      "    inference_res = r.json()\n",
      "  File \"C:\\Users\\arpit\\anaconda3\\lib\\site-packages\\requests\\models.py\", line 900, in json\n",
      "    return complexjson.loads(self.text, **kwargs)\n",
      "  File \"C:\\Users\\arpit\\anaconda3\\lib\\json\\__init__.py\", line 357, in loads\n",
      "    return _default_decoder.decode(s)\n",
      "  File \"C:\\Users\\arpit\\anaconda3\\lib\\json\\decoder.py\", line 337, in decode\n",
      "    obj, end = self.raw_decode(s, idx=_w(s, 0).end())\n",
      "  File \"C:\\Users\\arpit\\anaconda3\\lib\\json\\decoder.py\", line 355, in raw_decode\n",
      "    raise JSONDecodeError(\"Expecting value\", s, err.value) from None\n",
      "json.decoder.JSONDecodeError: Expecting value: line 1 column 1 (char 0)\n",
      " 60%|█████████████████████████████████████████████████████████                                      | 6/10 [49:05<32:39, 489.92s/it]Traceback (most recent call last):\n",
      "  File \"<ipython-input-9-dda364e21729>\", line 22, in process_results\n",
      "    inference_res = get_inference_results(csv_path_test)\n",
      "  File \"<ipython-input-7-488d7d0cc65c>\", line 8, in get_inference_results\n",
      "    inference_res = r.json()\n",
      "  File \"C:\\Users\\arpit\\anaconda3\\lib\\site-packages\\requests\\models.py\", line 900, in json\n",
      "    return complexjson.loads(self.text, **kwargs)\n",
      "  File \"C:\\Users\\arpit\\anaconda3\\lib\\json\\__init__.py\", line 357, in loads\n",
      "    return _default_decoder.decode(s)\n",
      "  File \"C:\\Users\\arpit\\anaconda3\\lib\\json\\decoder.py\", line 337, in decode\n",
      "    obj, end = self.raw_decode(s, idx=_w(s, 0).end())\n",
      "  File \"C:\\Users\\arpit\\anaconda3\\lib\\json\\decoder.py\", line 355, in raw_decode\n",
      "    raise JSONDecodeError(\"Expecting value\", s, err.value) from None\n",
      "json.decoder.JSONDecodeError: Expecting value: line 1 column 1 (char 0)\n",
      " 70%|██████████████████████████████████████████████████████████████████▌                            | 7/10 [57:08<24:22, 487.63s/it]Traceback (most recent call last):\n",
      "  File \"<ipython-input-9-dda364e21729>\", line 22, in process_results\n",
      "    inference_res = get_inference_results(csv_path_test)\n",
      "  File \"<ipython-input-7-488d7d0cc65c>\", line 8, in get_inference_results\n",
      "    inference_res = r.json()\n",
      "  File \"C:\\Users\\arpit\\anaconda3\\lib\\site-packages\\requests\\models.py\", line 900, in json\n",
      "    return complexjson.loads(self.text, **kwargs)\n",
      "  File \"C:\\Users\\arpit\\anaconda3\\lib\\json\\__init__.py\", line 357, in loads\n",
      "    return _default_decoder.decode(s)\n",
      "  File \"C:\\Users\\arpit\\anaconda3\\lib\\json\\decoder.py\", line 337, in decode\n",
      "    obj, end = self.raw_decode(s, idx=_w(s, 0).end())\n",
      "  File \"C:\\Users\\arpit\\anaconda3\\lib\\json\\decoder.py\", line 355, in raw_decode\n",
      "    raise JSONDecodeError(\"Expecting value\", s, err.value) from None\n",
      "json.decoder.JSONDecodeError: Expecting value: line 1 column 1 (char 0)\n",
      " 80%|██████████████████████████████████████████████████████████████████████████▍                  | 8/10 [1:05:10<16:11, 485.74s/it]Traceback (most recent call last):\n",
      "  File \"<ipython-input-9-dda364e21729>\", line 22, in process_results\n",
      "    inference_res = get_inference_results(csv_path_test)\n",
      "  File \"<ipython-input-7-488d7d0cc65c>\", line 8, in get_inference_results\n",
      "    inference_res = r.json()\n",
      "  File \"C:\\Users\\arpit\\anaconda3\\lib\\site-packages\\requests\\models.py\", line 900, in json\n",
      "    return complexjson.loads(self.text, **kwargs)\n",
      "  File \"C:\\Users\\arpit\\anaconda3\\lib\\json\\__init__.py\", line 357, in loads\n",
      "    return _default_decoder.decode(s)\n",
      "  File \"C:\\Users\\arpit\\anaconda3\\lib\\json\\decoder.py\", line 337, in decode\n",
      "    obj, end = self.raw_decode(s, idx=_w(s, 0).end())\n",
      "  File \"C:\\Users\\arpit\\anaconda3\\lib\\json\\decoder.py\", line 355, in raw_decode\n",
      "    raise JSONDecodeError(\"Expecting value\", s, err.value) from None\n",
      "json.decoder.JSONDecodeError: Expecting value: line 1 column 1 (char 0)\n",
      " 90%|███████████████████████████████████████████████████████████████████████████████████▋         | 9/10 [1:13:32<08:11, 491.04s/it]Traceback (most recent call last):\n",
      "  File \"<ipython-input-9-dda364e21729>\", line 22, in process_results\n",
      "    inference_res = get_inference_results(csv_path_test)\n",
      "  File \"<ipython-input-7-488d7d0cc65c>\", line 8, in get_inference_results\n",
      "    inference_res = r.json()\n",
      "  File \"C:\\Users\\arpit\\anaconda3\\lib\\site-packages\\requests\\models.py\", line 900, in json\n",
      "    return complexjson.loads(self.text, **kwargs)\n",
      "  File \"C:\\Users\\arpit\\anaconda3\\lib\\json\\__init__.py\", line 357, in loads\n",
      "    return _default_decoder.decode(s)\n",
      "  File \"C:\\Users\\arpit\\anaconda3\\lib\\json\\decoder.py\", line 337, in decode\n",
      "    obj, end = self.raw_decode(s, idx=_w(s, 0).end())\n",
      "  File \"C:\\Users\\arpit\\anaconda3\\lib\\json\\decoder.py\", line 355, in raw_decode\n",
      "    raise JSONDecodeError(\"Expecting value\", s, err.value) from None\n",
      "json.decoder.JSONDecodeError: Expecting value: line 1 column 1 (char 0)\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████| 10/10 [1:21:47<00:00, 490.72s/it]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████| 10/10 [57:39<00:00, 345.95s/it]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████| 10/10 [6:54:58<00:00, 2489.86s/it]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████| 10/10 [37:33<00:00, 225.40s/it]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████| 10/10 [4:15:33<00:00, 1533.38s/it]\n"
     ]
    }
   ],
   "source": [
    "def process_results(data_directory, dataset, seed, sep=','):\n",
    "    proc, gateway_proc, backend_proc, details = None, None, None, {}\n",
    "    try:\n",
    "        gateway_proc = subprocess.Popen(['python', os.path.join(data_directory, 'gateway_simulation.py')])\n",
    "        backend_proc = subprocess.Popen(['python', r\"C:\\Users\\arpit\\workspace\\MainAppCuda\\main.py\"])\n",
    "        proc = subprocess.Popen(['python', os.path.join(data_directory, 'sonar_stream_V2.py'), dataset, str(seed), \"\", sep])\n",
    "        \n",
    "        data_dir = r\"C:\\Users\\arpit\\workspace\\setup-stuff\\gateway_and_dataset\\Classification\\DATASETS_CLASSIFICATION\"\n",
    "        remove_split = \"_\".join(dataset.split(\"_\")[:-1])\n",
    "        csv_path_train = os.path.join(data_dir, dataset, remove_split + '_train' + str(seed) + '_mod.csv')\n",
    "        csv_path_test = os.path.join(data_dir, dataset, remove_split + '_test' + str(seed) + '_mod.csv')\n",
    "        csv_path_val = os.path.join(data_dir, dataset, remove_split + '_val' + str(seed) + '_mod.csv')\n",
    "        \n",
    "        train_df = pd.read_csv(csv_path_train, sep=sep)\n",
    "        target = list(train_df.iloc[:, -1].unique())\n",
    "        target = [str(tar) for tar in target]\n",
    "        \n",
    "        prereq_res = get_prereq_results(target)\n",
    "        phase0_res = get_phase0_results()\n",
    "        phase1_res = get_phase1_results(len(target), csv_path_train)\n",
    "        phase2_res = get_phase2_results(phase1_res[\"class\"], csv_path_val, csv_path_train, len(target))\n",
    "        inference_res = get_inference_results(csv_path_test)\n",
    "        \n",
    "        features = min(len(phase1_res[\"class\"][0][\"rankings\"]), 15)\n",
    "        details = {'dataset': dataset, 'seed': seed, 'KernelTime': phase2_res['kernelTime'], 'hostTime': phase2_res['hostTime']}\n",
    "        details['phase1_chunks_for_convergence'] = phase1_res['chunks_for_convergence']\n",
    "        details['phase2_chunks_for_convergence'] = phase2_res['chunks_for_convergence_phase2']\n",
    "        details['best_feature_list'] = phase2_res['minFeatureSpace']\n",
    "        details['best_lr'] = phase2_res['learning_rate']\n",
    "        details['model_id'] = phase2_res['model_index']\n",
    "        details['best_fs'] = len(phase2_res['minFeatureSpace'])\n",
    "        details['test_accuracy'] = inference_res['accuracyScore']\n",
    "        details['test_precision'] = inference_res['precision']\n",
    "        details['test_recall'] = inference_res['recall']\n",
    "        details['test_fscore'] = inference_res['fscore']\n",
    "        details['test_accuracies'] = inference_res['test_accuracies']\n",
    "        \n",
    "        details['train_accuracy'] = phase2_res['accuracyScore_t']\n",
    "        details['train_precision'] = phase2_res['precision_t']\n",
    "        details['train_recall'] = phase2_res['recall_t']\n",
    "        details['train_fscore'] = phase2_res['fscore_t']\n",
    "        details['val_accuracy'] = phase2_res['accuracyScore']\n",
    "        details['val_precision'] = phase2_res['precision']\n",
    "        details['val_recall'] = phase2_res['recall']\n",
    "        details['val_fscore'] = phase2_res['fscore']\n",
    "        \n",
    "        imp_features = []\n",
    "        for k in range(2):\n",
    "            for i in range(features):\n",
    "                imp_features.append(phase1_res[\"class\"][k][\"rankings\"][i]['Feature'])\n",
    "            details['important_features_class_'+ str(k)] = imp_features\n",
    "            imp_features = []\n",
    "        if backend_proc:\n",
    "            backend_proc.kill()\n",
    "        if proc:\n",
    "            proc.kill()\n",
    "        if gateway_proc:\n",
    "            gateway_proc.kill()\n",
    "    except:\n",
    "        traceback.print_exc()\n",
    "        if proc:\n",
    "            proc.kill()\n",
    "        if gateway_proc:\n",
    "            gateway_proc.kill()\n",
    "        if backend_proc:\n",
    "            backend_proc.kill()\n",
    "    return details\n",
    "\n",
    "\n",
    "data_dir =\\\n",
    "    r\"C:\\Users\\arpit\\workspace\\setup-stuff\\gateway_and_dataset\"\n",
    "# datasets = ['data15_occupanydetection_splits','data19_musk_splits','data26_sales_splits','data2_letter_splits',\n",
    "#             'data12_srbct_splits', 'data27_yeast_splits', 'data7_penbased_splits', 'data13_lymphoma_splits',\n",
    "#             'data17_skinseg_splits', 'data6_shuttle_splits', 'data16_poker_splits', 'data1_credit_splits',\n",
    "#             'data20_carvana_splits', #-> Taking too long, Skipping it. 179k samples in training\n",
    "#             'data23_forestcover_splits', #-> Taking too long, Skipping it. 154k samples in training\n",
    "datasets = ['data24_voice_splits','data28_theorem_splits','data4_activity_splits', 'data8_income_splits',\n",
    "            'data18_challenges_splits', 'data21_gimmecredit_splits', 'data22_connect_splits',\n",
    "            'data5_bank_splits', 'data29_dota_splits']\n",
    "# 'data25_arrythmia_splits' ,'data9_amlall_splits', 'data10_brain_splits','data14_prostrate_splits', 'data11_cns_splits',\n",
    "\n",
    "seeds = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
    "\n",
    "for i in range(len(datasets)):\n",
    "    df = pd.DataFrame()\n",
    "    for seed in tqdm(seeds):\n",
    "        dataset = datasets[i]\n",
    "        details = process_results(data_dir, dataset, seed)\n",
    "        df = df.append(details, ignore_index=True)\n",
    "        filepath = Path(dataset + '_manasa.csv')\n",
    "        filepath.parent.mkdir(parents=True, exist_ok=True)\n",
    "        df.to_csv(filepath, index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b168777c-3879-448d-898e-a236fe8cb8e4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
